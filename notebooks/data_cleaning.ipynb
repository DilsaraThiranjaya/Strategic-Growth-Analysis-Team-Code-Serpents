{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686db02c",
   "metadata": {},
   "source": [
    "# Phase 1: Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d5817",
   "metadata": {},
   "source": [
    "## Task 1: Load Data\n",
    "\n",
    "### Explanation:\n",
    "First, I bring in the libraries: Pandas for handling data like a table, and NumPy for math stuff (though Pandas uses it behind the scenes). I load the CSV file into a DataFrame called \"df\" – think of it as a spreadsheet in Python. To make it faster for a huge file (over 1 million rows), I tell Pandas the data types ahead of time. This saves memory and speeds things up – a smart trick pros use. I add \"encoding=`ISO-8859-1` to handle any weird characters. Finally, I print a message to confirm it's loaded and show the size (rows and columns). This keeps things clear and helps spot issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c629563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded successfully! \n",
      "   Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "           InvoiceDate  Price  Customer ID         Country  \n",
      "0  2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1  2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2  2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3  2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4  2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = './../data/online_retail.csv'\n",
    "\n",
    "d_types = {\n",
    "    'Invoice': 'object',\n",
    "    'StockCode': 'object',\n",
    "    'Description': 'object',\n",
    "    'Quantity': 'int64',\n",
    "    'InvoiceDate': 'object',\n",
    "    'Price': 'float64',\n",
    "    'Customer ID': 'float64',\n",
    "    'Country': 'object'\n",
    "}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path, dtype=d_types, encoding=\"ISO-8859-1\")\n",
    "    print(f\"Data Loaded successfully! \\n {df.head()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Data type mismatch : {ve}\")\n",
    "except EncodingWarning as ew:\n",
    "    print(f\"Encoding warning : {ew}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error : {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8566814",
   "metadata": {},
   "source": [
    "## Task 2: Initial Assessment\n",
    "\n",
    "### Explanation:\n",
    "Here, I take a quick look at the data to understand what's inside. I use \".info()\" to see data types (like numbers or text) and how many values are filled. \".describe(include='all')\" gives summaries, like averages for numbers or common values for text. And \".isnull().sum()\" counts empty spots in each column. This is like a health check – it helps us find problems like missing info or wrong formats right away. I print everything neatly so it's easy to read. It's a best practice to do this early, without changing the data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3c9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DataFrame Information ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1067371 entries, 0 to 1067370\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   Invoice      1067371 non-null  object \n",
      " 1   StockCode    1067371 non-null  object \n",
      " 2   Description  1062989 non-null  object \n",
      " 3   Quantity     1067371 non-null  int64  \n",
      " 4   InvoiceDate  1067371 non-null  object \n",
      " 5   Price        1067371 non-null  float64\n",
      " 6   Customer ID  824364 non-null   float64\n",
      " 7   Country      1067371 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 65.1+ MB\n",
      "None\n",
      "\n",
      "=== Statistical Summary ===\n",
      "        Invoice StockCode                         Description      Quantity  \\\n",
      "count   1067371   1067371                             1062989  1.067371e+06   \n",
      "unique    53628      5305                                5698           NaN   \n",
      "top      537434    85123A  WHITE HANGING HEART T-LIGHT HOLDER           NaN   \n",
      "freq       1350      5829                                5918           NaN   \n",
      "mean        NaN       NaN                                 NaN  9.938898e+00   \n",
      "std         NaN       NaN                                 NaN  1.727058e+02   \n",
      "min         NaN       NaN                                 NaN -8.099500e+04   \n",
      "25%         NaN       NaN                                 NaN  1.000000e+00   \n",
      "50%         NaN       NaN                                 NaN  3.000000e+00   \n",
      "75%         NaN       NaN                                 NaN  1.000000e+01   \n",
      "max         NaN       NaN                                 NaN  8.099500e+04   \n",
      "\n",
      "                InvoiceDate         Price    Customer ID         Country  \n",
      "count               1067371  1.067371e+06  824364.000000         1067371  \n",
      "unique                47635           NaN            NaN              43  \n",
      "top     2010-12-06 16:57:00           NaN            NaN  United Kingdom  \n",
      "freq                   1350           NaN            NaN          981330  \n",
      "mean                    NaN  4.649388e+00   15324.638504             NaN  \n",
      "std                     NaN  1.235531e+02    1697.464450             NaN  \n",
      "min                     NaN -5.359436e+04   12346.000000             NaN  \n",
      "25%                     NaN  1.250000e+00   13975.000000             NaN  \n",
      "50%                     NaN  2.100000e+00   15255.000000             NaN  \n",
      "75%                     NaN  4.150000e+00   16797.000000             NaN  \n",
      "max                     NaN  3.897000e+04   18287.000000             NaN  \n",
      "\n",
      "=== Missing Values ===\n",
      "Invoice             0\n",
      "StockCode           0\n",
      "Description      4382\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "Price               0\n",
      "Customer ID    243007\n",
      "Country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== DataFrame Information ===\")\n",
    "print(df.info())\n",
    "    \n",
    "print(f\"\\n=== Statistical Summary ===\")\n",
    "print(df.describe(include='all'))\n",
    "    \n",
    "print(f\"\\n=== Missing Values ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b913df",
   "metadata": {},
   "source": [
    "## Task 3: Handle Duplicates\n",
    "\n",
    "### Explanation:\n",
    "Duplicates are rows that are exactly the same – they can mess up our analysis by making sales look higher than they are. I count them first with \".duplicated().sum()\", then remove them using \".drop_duplicates()\". I explain why in the comment: it avoids bias, like overcounting hot items. I print the count and the new size to track changes. This is safe and clear – I re-assign to \"df\" to avoid surprises, which is a good Python habit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7913d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows are: 34335\n",
      "Duplicate values removed. New shape is: (1033036, 8)\n"
     ]
    }
   ],
   "source": [
    "duplicates_count = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows are: {duplicates_count}\")\n",
    "df = df.drop_duplicates()  \n",
    "print(f\"Duplicate values removed. New shape is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b522241",
   "metadata": {},
   "source": [
    "## Task 4: Handle Missing Data\n",
    "\n",
    "### Explanation:\n",
    "The big problem is missing Customer IDs – I can't analyze customers without them. I decide to remove those rows and explain why: it's better for quality, even if I lose some data (about 25% here), because keeping them would mess up grouping later. I count them, remove with \".dropna()\", and print updates. I also clean up any missing descriptions the same way, since they're not super important but could cause issues. This shows thoughtful cleaning – I justify choices to avoid bias and keep the data reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf462f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing customer id : 235151\n",
      "Remove misssing customer id rows. New shape is : (797885, 8)\n",
      "Invoice        0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "Price          0\n",
      "Customer ID    0\n",
      "Country        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_cusid = df['Customer ID'].isnull().sum()\n",
    "print(f\"Number of rows missing customer id : {missing_cusid}\")\n",
    "df = df.dropna(subset=['Customer ID'])\n",
    "print(f\"Remove misssing customer id rows. New shape is : {df.shape}\")\n",
    "\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8667388",
   "metadata": {},
   "source": [
    "## Task 5: Clean Transactional Data\n",
    "\n",
    "### Explanation:\n",
    "I clean the sales data in parts. First, remove cancelled orders (starting with 'C') because they don't add to sales and can make numbers look bad. Second, drop zero-price items – they're not real sales, like freebies. Third, filter out non-products like postage fees, as they skew product analysis. I also remove negative quantities (returns) for the same reason. For each, I count, remove, justify, and print changes. This makes the code easy to follow and ensures clean, unbiased data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4787472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of cancelled orders: 18390\n",
      "Remove cancelled orders. New shape is : (779495, 8)\n"
     ]
    }
   ],
   "source": [
    "cancellsd_order_count = df[df['Invoice'].str.startswith('C', na=False)].shape[0]\n",
    "print(f\"\\nNumber of cancelled orders: {cancellsd_order_count}\")\n",
    "df = df[~df['Invoice'].str.startswith('C', na=False)]\n",
    "print(f\"Remove cancelled orders. New shape is : {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90658b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of zero price rows: 70\n",
      "Zero price rows removed. New shape: (779425, 8)\n"
     ]
    }
   ],
   "source": [
    "zero_count = df[df['Price'] == 0].shape[0]\n",
    "print(f\"\\nNumber of zero price rows: {zero_count}\")\n",
    "df = df[df['Price'] > 0]\n",
    "print(f\"Zero price rows removed. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd04616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of non product rows: 2779\n",
      "Non product rows removed. New shape: (776646, 8)\n"
     ]
    }
   ],
   "source": [
    "non_product_codes = ['POST', 'M', 'BANK CHARGES', 'C2', 'DOT', 'CRUK'] \n",
    "non_product_count = df[df['StockCode'].isin(non_product_codes)].shape[0]\n",
    "print(f\"\\nNumber of non product rows: {non_product_count}\")\n",
    "df = df[~df['StockCode'].isin(non_product_codes)]\n",
    "print(f\"Non product rows removed. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e571f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of negative or zero Quantity: 0\n",
      "Negative or zero quantity rows removed. New shape: (776646, 8)\n"
     ]
    }
   ],
   "source": [
    "zero_qty = df[df['Quantity'] <= 0].shape[0]\n",
    "print(f\"\\nNumber of negative or zero Quantity: {zero_qty}\")\n",
    "df = df[df['Quantity'] > 0]\n",
    "print(f\"Negative or zero quantity rows removed. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346dd08",
   "metadata": {},
   "source": [
    "## Task 6: Feature Engineering\n",
    "\n",
    "### Explanation:\n",
    "Now I add new helpful columns. \"TotalPrice\" is just quantity times price – that's the money from each sale. Then, I turn the date string into a real date format and pull out year, month, day of week, and hour. This makes time-based analysis easy later. I print samples to check it worked. It's fast and uses Pandas' built-in tools – no loops needed, which is efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3736b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TotalPrice column added. New shape: (776646, 9)\n",
      "   Quantity  Price  TotalPrice\n",
      "0        12   6.95        83.4\n",
      "1        12   6.75        81.0\n",
      "2        12   6.75        81.0\n",
      "3        48   2.10       100.8\n",
      "4        24   1.25        30.0\n"
     ]
    }
   ],
   "source": [
    "df['TotalPrice'] = df['Quantity'] * df['Price']\n",
    "print(f\"\\nTotalPrice column added. New shape: {df.shape}\")\n",
    "print(f\"{df[['Quantity', 'Price', 'TotalPrice']].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0fe57a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date columns added. New shape: (776646, 13)\n",
      "          InvoiceDate  Year  Month  DayOfWeek  HourOfDay\n",
      "0 2009-12-01 07:45:00  2009     12          1          7\n",
      "1 2009-12-01 07:45:00  2009     12          1          7\n",
      "2 2009-12-01 07:45:00  2009     12          1          7\n",
      "3 2009-12-01 07:45:00  2009     12          1          7\n",
      "4 2009-12-01 07:45:00  2009     12          1          7\n"
     ]
    }
   ],
   "source": [
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='ISO8601')\n",
    "df['Year'] = df['InvoiceDate'].dt.year\n",
    "df['Month'] = df['InvoiceDate'].dt.month\n",
    "df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek  \n",
    "df['HourOfDay'] = df['InvoiceDate'].dt.hour\n",
    "print(f\"\\nDate columns added. New shape: {df.shape}\")\n",
    "print(f\"{df[['InvoiceDate', 'Year', 'Month', 'DayOfWeek', 'HourOfDay']].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b4aeb",
   "metadata": {},
   "source": [
    "## Task 7: Data Type Conversion\n",
    "\n",
    "### Explanation:\n",
    "Finally, I fix the data types to make sure everything is correct and efficient. Customer ID becomes a whole number, stock code a string, and so on. I already handled the date. I print the final types and size to confirm. Saving the cleaned file is a pro move – it lets you or others reuse it without re-running everything. This wraps up the cleaning nicely, leaving the data ready and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1b36c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Data Types:\n",
      "Invoice                object\n",
      "StockCode              object\n",
      "Description            object\n",
      "Quantity                int64\n",
      "InvoiceDate    datetime64[ns]\n",
      "Price                 float32\n",
      "Customer ID             int64\n",
      "Country                object\n",
      "TotalPrice            float32\n",
      "Year                    int32\n",
      "Month                   int32\n",
      "DayOfWeek               int32\n",
      "HourOfDay               int32\n",
      "dtype: object\n",
      "\n",
      "Final Shape: (776646, 13)\n"
     ]
    }
   ],
   "source": [
    "df['Customer ID'] = df['Customer ID'].astype(int)\n",
    "df['StockCode'] = df['StockCode'].astype(str)\n",
    "df['Quantity'] = df['Quantity'].astype(int)\n",
    "df['Price'] = df['Price'].astype('float32')\n",
    "df['TotalPrice'] = df['TotalPrice'].astype('float32')\n",
    "\n",
    "print(f\"\\nFinal Data Types:\")\n",
    "print(f\"{df.dtypes}\")\n",
    "print(f\"\\nFinal Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99256af",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "### Explanation:\n",
    "I save the cleaned data to a new CSV file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10deb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'online_retail_clean.csv.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('../data/online_retail_clean.csv', index=False)\n",
    "print(f\"Cleaned data saved to 'online_retail_clean.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
